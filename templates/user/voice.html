<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <title>Voice Call Active</title>
    <link rel="manifest" href="./manifest.json" />
    <style>
      body {
        background-color: #01131c;
        color: #ffffff;
        font-family: "Segoe UI", Tahoma, Geneva, Verdana, sans-serif;
        display: flex;
        flex-direction: column;
        align-items: center;
        justify-content: center;
        height: 100vh;
        margin: 0;
        padding: 20px;
        text-align: center;
      }
      h1 {
        font-size: 2em;
        margin-bottom: 1em;
        color: #ff5800;
      }

      #status {
        font-size: 1.2em;
        margin-bottom: 2em;
        color: #ff8040;
      }

      #endCallBtn {
        background-color: #dc2626;
        color: #fff;
        border: none;
        padding: 12px 24px;
        font-size: 1em;
        border-radius: 8px;
        cursor: pointer;
        transition: background-color 0.3s ease;
        margin-bottom: 2em;
      }

      #endCallBtn:hover {
        background-color: #b91c1c;
      }

      #micStatus {
        margin-top: 1.5em;
        font-size: 1.1em;
      }

      /* Audio Circle Styles */
      #audioCircle {
        width: 100px;
        height: 100px;
        border-radius: 50%;
        background: radial-gradient(circle, #ff5800, #ff8040);
        box-shadow: 0 0 20px rgba(255, 88, 0, 0.5);
        transition: all 0.1s ease-out;
        margin: 2em 0;
        display: none;
        position: relative;
      }

      #audioCircle.active {
        display: block;
        animation: pulse 2s ease-in-out infinite;
      }

      #audioCircle::before {
        content: "";
        position: absolute;
        top: -10px;
        left: -10px;
        right: -10px;
        bottom: -10px;
        border-radius: 50%;
        background: rgba(255, 88, 0, 0.2);
        animation: ripple 1.5s ease-out infinite;
        transition: background 0.3s ease;
      }

      @keyframes pulse {
        0%,
        100% {
          opacity: 0.8;
        }
        50% {
          opacity: 1;
        }
      }

      @keyframes ripple {
        0% {
          transform: scale(1);
          opacity: 0.6;
        }
        100% {
          transform: scale(2);
          opacity: 0;
        }
      }

      #volumeLevel {
        margin-top: 1em;
        font-size: 0.9em;
        color: #ff8040;
      }

      .loading {
        display: inline-block;
        width: 20px;
        height: 20px;
        border: 2px solid #ff8040;
        border-radius: 50%;
        border-top-color: transparent;
        animation: spin 1s ease-in-out infinite;
        margin-right: 10px;
        vertical-align: middle;
      }

      @keyframes spin {
        to {
          transform: rotate(360deg);
        }
      }
    </style>
  </head>
  <body>
    <h1>Voice Call</h1>
    <div id="status">
      <span class="loading"></span>
      Initializing call...
    </div>
    <button id="endCallBtn" style="display: none">End Call</button>
    <!-- Audio-reactive circle -->
    <div id="audioCircle"></div>
    <div id="volumeLevel"></div>
    <p id="micStatus"></p>
    <script>
      // Register Service Worker
      if ("serviceWorker" in navigator) {
        navigator.serviceWorker
          .register("./service-worker.js")
          .then(() => console.log("‚úÖ Service Worker Registered"))
          .catch((err) =>
            console.error("‚ùå Service Worker registration failed:", err),
          );
      }
    </script>
    <script>
      let micStream = null;
      let audioContext = null;
      let micAnalyser = null;
      let outputAnalyser = null;
      let micDataArray = null;
      let outputDataArray = null;
      let animationId = null;
      let callId = null;
      let isCallActive = false;

      const audioCircle = document.getElementById("audioCircle");
      const volumeLevel = document.getElementById("volumeLevel");
      const statusDiv = document.getElementById("status");
      const endCallBtn = document.getElementById("endCallBtn");

      // Auto-start the call when page loads
      window.addEventListener("load", async () => {
        await startCall();
      });

      async function startCall() {
        try {
          statusDiv.innerHTML = '<span class="loading"></span>Starting call...';

          const response = await fetch(
            "{{settings['backend_url']}}/min/start-call",
            {
              method: "POST",
              headers: {
                "Content-Type": "application/json",
              },
              body: JSON.stringify({
                assistant_id: "7e92b9db-f97e-431e-9b6d-3eb52be04999",
              }),
            },
          );

          const data = await response.json();
          console.log("Response:", data);

          if (data.status === "success") {
            callId = data.call_id || data.id; // Store call ID for cleanup
            isCallActive = true;
            statusDiv.innerHTML = "‚úÖ Call Active";
            endCallBtn.style.display = "inline-block";
            document.getElementById("micStatus").innerText =
              "üîÑ Getting microphone access...";
            await getMicAccess();
          } else {
            statusDiv.innerHTML = "‚ùå Failed to start call";
            document.getElementById("micStatus").innerText =
              "‚ùå Error: " + data.message;
          }
        } catch (error) {
          console.error("Fetch error:", error);
          statusDiv.innerHTML = "‚ùå Connection failed";
          document.getElementById("micStatus").innerText =
            "‚ùå Failed to start call";
        }
      }

      async function endCall() {
        if (!isCallActive) return;

        try {
          statusDiv.innerHTML = '<span class="loading"></span>Ending call...';

          // Stop all audio analysis and streams first
          stopAudioAnalysis();
          if (micStream) {
            micStream.getTracks().forEach((track) => track.stop());
            micStream = null;
          }

          // Call the backend to end the call
          const response = await fetch(
            "{{settings['backend_url']}}/min/end-call",
            {
              method: "POST",
              headers: {
                "Content-Type": "application/json",
              },
              body: JSON.stringify({
                call_id: callId,
              }),
            },
          );

          const data = await response.json();
          console.log("End call response:", data);

          isCallActive = false;
          statusDiv.innerHTML = "üìû Call Ended";
          endCallBtn.style.display = "none";
          document.getElementById("micStatus").innerText = "Call terminated";
        } catch (error) {
          console.error("Error ending call:", error);
          statusDiv.innerHTML = "‚ö†Ô∏è Call disconnected";
          isCallActive = false;
        }
      }

      // End call button event listener
      endCallBtn.addEventListener("click", endCall);

      async function getMicAccess() {
        try {
          micStream = await navigator.mediaDevices.getUserMedia({
            audio: true,
          });
          document.getElementById("micStatus").innerText =
            "üé§ Microphone access granted and stream is active.";
          console.log("Mic stream started:", micStream);

          // Set up audio analysis for both input and output
          setupAudioAnalysis();
        } catch (error) {
          document.getElementById("micStatus").innerText =
            "‚ùå Microphone access denied.";
          console.error("Microphone access error:", error);
        }
      }

      function setupAudioAnalysis() {
        // Create audio context
        audioContext = new (window.AudioContext || window.webkitAudioContext)();

        // Set up microphone analyser
        micAnalyser = audioContext.createAnalyser();
        micAnalyser.fftSize = 256;
        micAnalyser.smoothingTimeConstant = 0.8;

        const micBufferLength = micAnalyser.frequencyBinCount;
        micDataArray = new Uint8Array(micBufferLength);

        // Connect microphone to analyser
        const micSource = audioContext.createMediaStreamSource(micStream);
        micSource.connect(micAnalyser);

        // Set up output audio capture (for AI voice)
        setupOutputAudioCapture();

        // Show the circle and start animation
        audioCircle.classList.add("active");
        animateCircle();
      }

      async function setupOutputAudioCapture() {
        try {
          // Method 1: Try to capture system audio via getDisplayMedia
          const displayStream = await navigator.mediaDevices.getDisplayMedia({
            audio: {
              echoCancellation: false,
              noiseSuppression: false,
              sampleRate: 44100,
            },
            video: false,
          });

          // Set up output analyser
          outputAnalyser = audioContext.createAnalyser();
          outputAnalyser.fftSize = 256;
          outputAnalyser.smoothingTimeConstant = 0.8;

          const outputBufferLength = outputAnalyser.frequencyBinCount;
          outputDataArray = new Uint8Array(outputBufferLength);

          // Connect system audio to analyser
          const outputSource =
            audioContext.createMediaStreamSource(displayStream);
          outputSource.connect(outputAnalyser);

          console.log("‚úÖ System audio capture set up successfully");
        } catch (error) {
          console.log(
            "System audio not available, trying alternative methods...",
          );

          // Method 2: Try Web Audio API destination capture
          try {
            await setupDestinationCapture();
          } catch (destError) {
            console.log(
              "Destination capture failed, using audio element monitoring",
            );
            // Method 3: Fallback to audio element monitoring
            setupAudioElementCapture();
            setupWebRTCCapture();
          }
        }
      }

      async function setupDestinationCapture() {
        // Create a gain node to tap into the audio destination
        const destinationAnalyser = audioContext.createAnalyser();
        destinationAnalyser.fftSize = 256;
        destinationAnalyser.smoothingTimeConstant = 0.8;

        // Try to connect to the audio context destination
        const gainNode = audioContext.createGain();
        gainNode.connect(audioContext.destination);
        gainNode.connect(destinationAnalyser);

        outputAnalyser = destinationAnalyser;
        outputDataArray = new Uint8Array(outputAnalyser.frequencyBinCount);

        console.log("‚úÖ Audio destination capture set up");
      }

      function setupWebRTCCapture() {
        // Monitor for WebRTC audio streams (common in voice calling apps)
        const originalGetUserMedia = navigator.mediaDevices.getUserMedia;
        const originalGetDisplayMedia = navigator.mediaDevices.getDisplayMedia;

        // Intercept getUserMedia calls
        navigator.mediaDevices.getUserMedia = async function (constraints) {
          const stream = await originalGetUserMedia.call(this, constraints);

          // Check if this might be an outgoing audio stream
          if (constraints.audio && stream.getAudioTracks().length > 0) {
            console.log("Detected potential outgoing audio stream");
            tryConnectStream(stream);
          }

          return stream;
        };

        // Monitor RTCPeerConnection for remote streams
        const originalRTCPeerConnection = window.RTCPeerConnection;
        window.RTCPeerConnection = function (config) {
          const pc = new originalRTCPeerConnection(config);

          pc.addEventListener("track", (event) => {
            console.log("WebRTC track detected:", event.track.kind);
            if (event.track.kind === "audio") {
              const stream = new MediaStream([event.track]);
              tryConnectStream(stream, "WebRTC Remote");
            }
          });

          return pc;
        };
      }

      function tryConnectStream(stream, source = "Unknown") {
        if (!audioContext || !stream.getAudioTracks().length) return;

        try {
          if (!outputAnalyser) {
            outputAnalyser = audioContext.createAnalyser();
            outputAnalyser.fftSize = 256;
            outputAnalyser.smoothingTimeConstant = 0.8;
            outputDataArray = new Uint8Array(outputAnalyser.frequencyBinCount);
          }

          const source_node = audioContext.createMediaStreamSource(stream);
          source_node.connect(outputAnalyser);

          console.log(`‚úÖ Connected ${source} audio stream to analyser`);
        } catch (error) {
          console.log(`Failed to connect ${source} stream:`, error);
        }
      }

      function setupAudioElementCapture() {
        // Listen for any audio elements that might be created for AI voice
        const observer = new MutationObserver((mutations) => {
          mutations.forEach((mutation) => {
            mutation.addedNodes.forEach((node) => {
              if (node.tagName === "AUDIO" || node.tagName === "VIDEO") {
                console.log("üéµ New audio element detected:", node);
                connectAudioElement(node);
              }
              // Check for nested audio elements
              if (node.querySelectorAll) {
                node.querySelectorAll("audio, video").forEach((audioEl) => {
                  console.log("üéµ Nested audio element detected:", audioEl);
                  connectAudioElement(audioEl);
                });
              }
            });
          });
        });

        observer.observe(document.body, {
          childList: true,
          subtree: true,
          attributes: true,
          attributeFilter: ["src", "srcObject"],
        });

        // Also check existing audio elements
        document.querySelectorAll("audio, video").forEach((element) => {
          console.log("üéµ Existing audio element found:", element);
          connectAudioElement(element);
        });

        // Listen for audio play events on the entire document
        document.addEventListener(
          "play",
          (event) => {
            if (
              event.target.tagName === "AUDIO" ||
              event.target.tagName === "VIDEO"
            ) {
              console.log("üîä Audio play event detected:", event.target);
              connectAudioElement(event.target);
            }
          },
          true,
        );

        // Monitor for VAPI-specific audio patterns
        setupVAPIAudioMonitoring();
      }

      function setupVAPIAudioMonitoring() {
        // VAPI might use Web Audio API or MediaSource
        // Monitor for audio context creation
        const originalAudioContext =
          window.AudioContext || window.webkitAudioContext;
        const audioContexts = [];

        window.AudioContext = window.webkitAudioContext = function (...args) {
          const ctx = new originalAudioContext(...args);
          audioContexts.push(ctx);
          console.log("üéß New AudioContext created, monitoring for VAPI audio");

          // Hook into the destination to capture output
          const originalConnect = ctx.destination.connect;
          ctx.destination.connect = function (...args) {
            console.log("üîó Audio connection to destination detected");
            return originalConnect.apply(this, args);
          };

          return ctx;
        };

        // Monitor for MediaSource usage (streaming audio)
        const originalMediaSource = window.MediaSource;
        if (originalMediaSource) {
          window.MediaSource = function (...args) {
            const ms = new originalMediaSource(...args);
            console.log("üì∫ MediaSource created - potential streaming audio");

            ms.addEventListener("sourceopen", () => {
              console.log(
                "üì∫ MediaSource opened - audio streaming likely active",
              );
            });

            return ms;
          };
        }

        // Check for VAPI-specific elements or classes
        const checkForVAPIElements = () => {
          const vapiElements = document.querySelectorAll(
            '[class*="vapi"], [id*="vapi"], [data-vapi]',
          );
          vapiElements.forEach((el) => {
            const audioElements = el.querySelectorAll("audio, video");
            audioElements.forEach(connectAudioElement);
          });

          // Check for shadow DOM elements
          document.querySelectorAll("*").forEach((el) => {
            if (el.shadowRoot) {
              el.shadowRoot
                .querySelectorAll("audio, video")
                .forEach(connectAudioElement);
            }
          });
        };

        // Periodically check for new VAPI elements
        setInterval(checkForVAPIElements, 2000);
      }

      function connectAudioElement(audioElement) {
        if (!audioContext || !audioElement) return;

        try {
          // Avoid connecting the same element multiple times
          if (audioElement._vapiConnected) return;
          audioElement._vapiConnected = true;

          // Set up output analyser if not already done
          if (!outputAnalyser) {
            outputAnalyser = audioContext.createAnalyser();
            outputAnalyser.fftSize = 256;
            outputAnalyser.smoothingTimeConstant = 0.8;
            outputDataArray = new Uint8Array(outputAnalyser.frequencyBinCount);
          }

          // Connect audio element to analyser
          const source = audioContext.createMediaElementSource(audioElement);
          source.connect(outputAnalyser);
          source.connect(audioContext.destination); // Still play the audio

          console.log("‚úÖ Connected audio element to analyser:", audioElement);

          // Listen for audio events
          audioElement.addEventListener("play", () => {
            console.log("üîä AI audio started playing");
          });

          audioElement.addEventListener("ended", () => {
            console.log("üîá AI audio ended");
          });

          // Set CORS settings if possible
          if (audioElement.crossOrigin !== undefined) {
            audioElement.crossOrigin = "anonymous";
          }
        } catch (error) {
          console.log("Could not connect audio element:", error);
          // Try alternative connection method
          tryAlternativeConnection(audioElement);
        }
      }

      function tryAlternativeConnection(audioElement) {
        // Alternative method using audio worklet or script processor
        try {
          if (audioContext.createScriptProcessor) {
            const processor = audioContext.createScriptProcessor(2048, 1, 1);

            processor.onaudioprocess = (event) => {
              const input = event.inputBuffer.getChannelData(0);

              // Analyze the audio data manually
              let sum = 0;
              for (let i = 0; i < input.length; i++) {
                sum += Math.abs(input[i]);
              }
              const average = (sum / input.length) * 255;

              // Store in a global variable for the animation loop
              window.vapiAudioLevel = average;
            };

            // Try to connect (might fail due to CORS)
            const source = audioContext.createMediaElementSource(audioElement);
            source.connect(processor);
            processor.connect(audioContext.destination);

            console.log("‚úÖ Alternative audio connection established");
          }
        } catch (altError) {
          console.log("Alternative connection also failed:", altError);
        }
      }

      function animateCircle() {
        if (!micAnalyser) return;

        // Get microphone audio data
        micAnalyser.getByteFrequencyData(micDataArray);

        // Calculate average volume from microphone
        let micSum = 0;
        for (let i = 0; i < micDataArray.length; i++) {
          micSum += micDataArray[i];
        }
        const micAverage = micSum / micDataArray.length;

        // Get output audio data (AI voice) if available
        let outputAverage = 0;
        if (outputAnalyser && outputDataArray) {
          outputAnalyser.getByteFrequencyData(outputDataArray);
          let outputSum = 0;
          for (let i = 0; i < outputDataArray.length; i++) {
            outputSum += outputDataArray[i];
          }
          outputAverage = outputSum / outputDataArray.length;
        }

        // Check for alternative audio level (from script processor)
        if (window.vapiAudioLevel && window.vapiAudioLevel > outputAverage) {
          outputAverage = window.vapiAudioLevel;
          // Decay the stored level
          window.vapiAudioLevel *= 0.95;
        }

        // Determine which source is active (user or AI)
        const activeSource = micAverage > outputAverage ? "user" : "ai";
        const activeVolume =
          activeSource === "user" ? micAverage : outputAverage;

        // Normalize volume (0-100)
        const volumePercent = Math.round((activeVolume / 255) * 100);

        // Scale circle based on volume (50px to 200px)
        const minSize = 50;
        const maxSize = 200;
        const size = minSize + (activeVolume / 255) * (maxSize - minSize);

        // Change color based on audio source
        let circleColor, glowColor;
        if (activeSource === "user") {
          // User speaking - orange/red
          circleColor = "radial-gradient(circle, #ff5800, #ff8040)";
          glowColor = "rgba(255, 88, 0";
        } else {
          // AI speaking - blue/cyan
          circleColor = "radial-gradient(circle, #00a8ff, #40c8ff)";
          glowColor = "rgba(0, 168, 255";
        }

        // Apply size, color, and glow effect
        audioCircle.style.width = size + "px";
        audioCircle.style.height = size + "px";
        audioCircle.style.background = circleColor;
        audioCircle.style.boxShadow = `0 0 ${size / 3}px ${glowColor}, ${0.3 + (activeVolume / 255) * 0.7})`;

        // Update volume display with source info
        let sourceInfo = "";
        if (activeVolume > 10) {
          sourceInfo = activeSource === "user" ? " (You)" : " (AI)";
        }
        volumeLevel.textContent = `Volume: ${volumePercent}%${sourceInfo}`;

        // Continue animation
        animationId = requestAnimationFrame(animateCircle);
      }

      function stopAudioAnalysis() {
        if (animationId) {
          cancelAnimationFrame(animationId);
          animationId = null;
        }

        if (audioContext && audioContext.state !== "closed") {
          audioContext.close();
          audioContext = null;
        }

        audioCircle.classList.remove("active");
        volumeLevel.textContent = "";
      }

      // Handle page close/refresh - cleanup call and connections
      window.addEventListener("beforeunload", async (e) => {
        if (isCallActive) {
          // Attempt to end the call gracefully
          navigator.sendBeacon(
            "{{settings['backend_url']}}/min/end-call",
            JSON.stringify({
              call_id: callId,
            }),
          );
        }

        // Stop all audio streams and analysis
        stopAudioAnalysis();
        if (micStream) {
          micStream.getTracks().forEach((track) => track.stop());
        }
      });

      // Handle page visibility change (when tab becomes hidden/visible)
      document.addEventListener("visibilitychange", () => {
        if (document.hidden && isCallActive) {
          console.log("Page hidden - call still active");
        } else if (!document.hidden && isCallActive) {
          console.log("Page visible - call active");
        }
      });

      // Handle browser/tab close via pagehide event (more reliable than beforeunload)
      window.addEventListener("pagehide", () => {
        if (isCallActive && callId) {
          // Use sendBeacon for reliable delivery even during page unload
          navigator.sendBeacon(
            "{{settings['backend_url']}}/min/end-call",
            JSON.stringify({
              call_id: callId,
            }),
          );
        }
      });
    </script>
  </body>
</html>
